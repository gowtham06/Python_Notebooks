{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_name='review_text.txt'\n",
    "current_dir= os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "# DATA_DIR='/Users/gowthamkannan/Desktop/Assignments/Computational_Linguistics/Assignment_2/pos_tagger/data'\n",
    "# DATA_FILE='dialogues.txt'\n",
    "ftrain = open(os.path.join(current_dir, file_name), 'rb')\n",
    "label_list=[]\n",
    "for line in ftrain:\n",
    "    words=[]\n",
    "#     sentence=line.strip().split('\\t')[0]\n",
    "#     label=line.strip().split('\\t')[1]\n",
    "    sentence=line.strip().split(b'DELIMETER')[0]\n",
    "    label=line.strip().split(b'DELIMETER')[1]\n",
    "    \n",
    "#     print(label,sentence)\n",
    "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
    "    label=label.decode(\"ascii\",\"ignore\").lower()\n",
    "    label_list.append(label)\n",
    "#     for item in words_temp:\n",
    "#         if item not in stopwords_list:\n",
    "#             words.append(item)\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    num_recs += 1\n",
    "# print(word_freqs)\n",
    "label_list=list(set(label_list))\n",
    "nos_classes=len(label_list)\n",
    "# print(len(label_list))\n",
    "ftrain.close()\n",
    "MAX_FEATURES=5000\n",
    "MAX_SENTENCE_LENGTH=500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in\n",
    "enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "X = np.empty((num_recs, ), dtype=list)\n",
    "nos_classes=2\n",
    "y = np.zeros((num_recs, nos_classes))\n",
    "i = 0\n",
    "f_train = open(os.path.join(current_dir, file_name), 'rb')\n",
    "for line in f_train:\n",
    "#     label,sentence=line.strip().split('\\t')\n",
    "    words=[]\n",
    "    sentence=line.strip().split(b'DELIMETER')[0]\n",
    "    label=line.strip().split(b'DELIMETER')[1]\n",
    "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
    "    label=label.decode(\"ascii\",\"ignore\").lower()\n",
    "    label_list.append(label)\n",
    "#     for item in words_temp:\n",
    "#         if item not in stopwords_list:\n",
    "#             words.append(item)\n",
    "    seqs=[]\n",
    "    l=[0]*nos_classes\n",
    "    for word in words:\n",
    "        if word in word2index.keys():\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index['UNK'])\n",
    "    X[i]=seqs\n",
    "    l[label_list.index(label)]=1\n",
    "    y[i]=l\n",
    "    i+=1\n",
    "f_train.close()\n",
    "print(MAX_SENTENCE_LENGTH)\n",
    "\n",
    "X=sequence.pad_sequences(X,maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n"
     ]
    }
   ],
   "source": [
    "Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.2)\n",
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160064    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 100)          32100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 20, 32)            32032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 6, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               19300     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 343,798\n",
      "Trainable params: 343,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32, input_length=MAX_SENTENCE_LENGTH))\n",
    "model.add(Conv1D(filters=100, kernel_size=10, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Conv1D(filters=100, kernel_size=10, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Conv1D(filters=32, kernel_size=10, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      " - 333s - loss: 0.1958 - acc: 0.9240 - val_loss: 0.2203 - val_acc: 0.9111\n",
      "Epoch 2/3\n",
      " - 307s - loss: 0.1458 - acc: 0.9467 - val_loss: 0.2477 - val_acc: 0.9030\n",
      "Epoch 3/3\n",
      " - 309s - loss: 0.1017 - acc: 0.9654 - val_loss: 0.2691 - val_acc: 0.9047\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cb0fd997e92a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, validation_data=(Xtest, ytest), epochs=3, batch_size=75, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
