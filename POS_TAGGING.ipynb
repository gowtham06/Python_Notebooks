{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the data set file by scrapping nltk treebank tag annonated text. Two data files \"treebank_sents.txt\" and \"treebank_poss.txt\" to store sentences and related pos tags correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "def get_tagging_data(DATA_DIR):\n",
    "    open(os.path.join(DATA_DIR, \"treebank_sents.txt\"), \"w\").close()\n",
    "    open(os.path.join(DATA_DIR, \"treebank_poss.txt\"), \"w\").close()\n",
    "    fedata = open(os.path.join(DATA_DIR, \"treebank_sents.txt\"), \"a\")\n",
    "    ffdata = open(os.path.join(DATA_DIR, \"treebank_poss.txt\"), \"a\")\n",
    "    sents = nltk.corpus.treebank.tagged_sents()\n",
    "    for sent in sents:\n",
    "        words, poss = [], []\n",
    "        for word, pos in sent:\n",
    "            if pos == \"-NONE-\":\n",
    "                continue\n",
    "            words.append(word)\n",
    "            poss.append(pos)\n",
    "        fedata.write(\" \".join(words))\n",
    "        fedata.write('\\n')\n",
    "        ffdata.write(\" \".join(poss))\n",
    "        ffdata.write('\\n')\n",
    "    fedata.close()\n",
    "    ffdata.close()\n",
    "\n",
    "DATA_DIR= os.getcwd()\n",
    "get_tagging_data(DATA_DIR)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the necessary packages and fucntions needed for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gowthamkannan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/gowthamkannan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation,Dense,Dropout,RepeatVector,SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a dictionary for words and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentences(filename):\n",
    "    word_freqs = collections.Counter()\n",
    "    num_recs, maxlen = 0, 0\n",
    "    fin = open(filename, \"rb\")\n",
    "    for line in fin:\n",
    "        words = line.strip().lower().split()\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "        if len(words) > maxlen:\n",
    "            maxlen = len(words)\n",
    "        num_recs += 1\n",
    "    fin.close()\n",
    "    return word_freqs, maxlen, num_recs\n",
    "\n",
    "s_wordfreqs, s_maxlen, s_numrecs = parse_sentences(os.path.join(DATA_DIR, \"treebank_sents.txt\"))\n",
    "t_wordfreqs, t_maxlen, t_numrecs = parse_sentences(os.path.join(DATA_DIR, \"treebank_poss.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a word2index and index2word dicitonary for the embedding layer to use. Also definig max sequence length(max sentence length that we are dealing with),the number of pos tags(nos. of labels for teh netowork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN=250\n",
    "S_MAX_FEATURES=5000\n",
    "T_MAX_FEATURES=45\n",
    "S_VOCAB_SIZE=min(len(s_wordfreqs),S_MAX_FEATURES)+2\n",
    "s_word2index={x[0]:i+2 for i,x in enumerate(s_wordfreqs.most_common(S_MAX_FEATURES))}\n",
    "s_word2index['PAD']=0\n",
    "s_word2index['UNK']=1\n",
    "s_index2word={v:k for k,v in s_word2index.items()}\n",
    "t_vocabsize=len(t_wordfreqs)+1\n",
    "t_word2index={x[0]:i for i,x in enumerate(t_wordfreqs.most_common(T_MAX_FEATURES))}\n",
    "t_word2index['PAD']=0\n",
    "t_index2word={v:k for k,v in t_word2index.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the input and output  tensor for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tensor(filename,numrecs,word2index,max_len,make_categorical=False,num_classes=0):\n",
    "    data=np.empty((numrecs,),dtype=list)\n",
    "    fin=open(filename,\"rb\")\n",
    "    i=0\n",
    "    for line in fin:\n",
    "        wids=[]\n",
    "        for word in line.strip().lower().split():\n",
    "            if word in word2index.keys():\n",
    "                wids.append(word2index[word])\n",
    "            else:\n",
    "                wids.append(word2index['UNK'])\n",
    "            if make_categorical:\n",
    "                data[i]=np_utils.to_categorical(wids,num_classes=num_classes)\n",
    "            else:\n",
    "                data[i]=wids\n",
    "        i=i+1\n",
    "    fin.close()\n",
    "\n",
    "    pdata=sequence.pad_sequences(data,maxlen=max_len)\n",
    "    return pdata\n",
    "\n",
    "X=construct_tensor(os.path.join(DATA_DIR,\"treebank_sents.txt\"),s_numrecs,s_word2index,MAX_SEQ_LEN)\n",
    "Y=construct_tensor(os.path.join(DATA_DIR,\"treebank_poss.txt\"),t_numrecs,t_word2index,MAX_SEQ_LEN,True,t_vocabsize)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spiting the dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train,X_Test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intializing the reccurent neural network with the necessary activation functions etc. Also specifiy the size of the emebedding , output of the embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE=128\n",
    "HIDDEN_LAYER_SIZE=50\n",
    "BATCH_SIZE=32\n",
    "NUM_EPOCHS=2\n",
    "model=Sequential()\n",
    "model.add(Embedding(S_VOCAB_SIZE,EMBED_SIZE,input_length=MAX_SEQ_LEN))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(GRU(HIDDEN_LAYER_SIZE,dropout=0.0,recurrent_dropout=0.0))\n",
    "model.add(RepeatVector(MAX_SEQ_LEN))\n",
    "model.add(GRU(HIDDEN_LAYER_SIZE,dropout=0.0,recurrent_dropout=0.0,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(t_vocabsize)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/2\n",
      "3131/3131 [==============================] - 72s 23ms/step - loss: 0.3032 - acc: 0.6989 - val_loss: 0.2933 - val_acc: 0.9146\n",
      "Epoch 2/2\n",
      "3131/3131 [==============================] - 72s 23ms/step - loss: 0.2862 - acc: 0.9155 - val_loss: 0.2914 - val_acc: 0.9108\n",
      "783/783 [==============================] - 3s 4ms/step\n",
      "Test score:0.291,accuracy:0.911\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_Train,y_train,batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,validation_data=[X_Test,y_test])\n",
    "score,accuracy=model.evaluate(X_Test,y_test,batch_size=BATCH_SIZE)\n",
    "print(\"Test score:%.3f,accuracy:%.3f\" %(score,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ploting the performance of the model during trainig phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17517ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"how are you\"\n",
    "data=np.empty((1,),dtype=list)\n",
    "wids=[]\n",
    "for word in text.split():\n",
    "    if word in s_word2index.keys():\n",
    "        wids.append(s_word2index[word])\n",
    "    else:\n",
    "        wids.append(s_word2index['UNK'])\n",
    "data[0]=wids\n",
    "new_data=sequence.pad_sequences(data,maxlen=MAX_SEQ_LEN)\n",
    "y=model.predict(new_data)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
